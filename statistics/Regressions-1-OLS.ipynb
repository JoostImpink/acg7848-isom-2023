{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression analysis - Ordinary Least Squares (OLS)\n",
    "\n",
    "## Overview\n",
    "\n",
    "- Regression tests/maps relationships between variables.\n",
    "- The variable that is being explained (on the left) is the dependent variable; variables on the right are independent variables (or predictors)\n",
    "- OLS with one independent variable is called simple OLS, more than one independent variables is called multiple OLS\n",
    "- Itâ€™s a common practice to denote the outputs with ğ‘¦ and the inputs with ğ‘¥. If there are two or more independent variables, then they can be represented as the vector ğ± = (ğ‘¥â‚, â€¦, ğ‘¥áµ£), where ğ‘Ÿ is the number of inputs.\n",
    "- So, we use regression to answer how __several variables__ are related. It is also useful for forecasting/predicting\n",
    "\n",
    "Equation for univariate linear regression (simple OLS): y = ğ›½0 + ğ›½1ğ‘¥ + Îµ<br>\n",
    "\n",
    "Equation for multivariate linear regression (multiple OLS): y = ğ›½0 +ğ›½1ğ‘¥1 +ğ›½2ğ‘¥2 +...+ ğ›½ğ‘›ğ‘¥ğ‘› + Îµ<br>\n",
    "    \n",
    "#### Evaluation\n",
    "\n",
    "There are several ways to evaluate a model:\n",
    "- the t-statistic (=coefficient / standard error) and p-value of the independent variables \n",
    "- R-squared of the whole model (all independent variables combined): variance explained / total variance\n",
    "- Mean-square error (MSE): the average of the error terms squared\n",
    "\n",
    "There are many other metrics for regression, although these are the most commonly used. You can see the full list of regression metrics supported by the scikit-learn Python machine learning library here:\n",
    "\n",
    "https://towardsdatascience.com/ways-to-evaluate-regression-models-77a3ff45ba70\n",
    "\n",
    "#### OLS Assumptions\n",
    "    \n",
    "There are several assumptions that need to be satisfied for the OLS regression output to be reliable. \n",
    "The most important assumptions are (1) that the model is complete (no missing variables), and (2) the relationship is linear, and (3) that the independent variables are not correlated with the error term.\n",
    "\n",
    "OLS is used if the dependent variable is continuous. If it is binary (zero or 1), you would need logisitic regression or probit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodel OLS and scikit linear regression\n",
    "\n",
    "There are two main packages that can do OLS in Python: Statsmodel and Scikit-learn (sklearn). \n",
    "\n",
    "The difference between the two (from https://stats.stackexchange.com/questions/146804/difference-between-statsmodel-ols-and-scikit-linear-regression)\n",
    "\n",
    "> Statsmodels follows largely the traditional model where we want to know how well a given model fits the data, and what variables \"explain\" or affect the outcome, or what the size of the effect is. Scikit-learn follows the machine learning tradition where the main supported task is chosing the \"best\" model for prediction.\n",
    "\n",
    "> As a consequence, the emphasis in the supporting features of statsmodels is in analysing the training data which includes hypothesis tests and goodness-of-fit measures, while the emphasis in the supporting infrastructure in scikit-learn is on model selection for out-of-sample prediction and therefore cross-validation on \"test data\".\n",
    "\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's do an OLS regression in both packages. Let's do a regression to explain the market-to-book ratio with profitability (return on assets) and firm size as the independent variables.\n",
    "\n",
    "The market to book ratio (stock price divided by the per share book value of equity) tells us how much investors are willing to pay for each $1 in book value of equity. Book value of equity is the sum of capital raised for issuing shares plus retained earnings (profits minus distributions), treasury stock (stock buybacks) are deducted.\n",
    "\n",
    "So we will measure if the market value is driven by profitability and/or firm size (or both of course).\n",
    "\n",
    "#### sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#df = pd.read_excel(r'..\\datasets\\Compustat-Funda.xlsx',nrows= 1000)\n",
    "df = pd.read_excel(r'..\\datasets\\Compustat-Funda.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter: only keep if positive price, equity, assets and csho (number of shares outstanding)\n",
    "df = df[ ( df['prcc_f'] > 0) & ( df['ceq'] > 0) & ( df['at'] > 0) & (df['csho'] > 0 ) ]\n",
    "\n",
    "# add mtb, roa and size\n",
    "df['mtb'] = df['prcc_f'] * df['csho'] / df['ceq']\n",
    "df['roa'] = df['ni'] / df['at']\n",
    "df['size'] = np.log ( df['prcc_f'] * df['csho'] )\n",
    "\n",
    "# drop if any of these have missing values\n",
    "df = df.dropna(subset=['mtb', 'roa', 'size'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mtb'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['roa', 'size']]\n",
    "y = df['mtb']\n",
    "lm.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('intercept', lm.intercept_, 'coefficients', lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitted values\n",
    "yhat = lm.predict( x )\n",
    "# note that the fitted (or predicted) values are very large\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# model performance\n",
    "print('R-squared:', r2_score(y, yhat) )\n",
    "print('MSE:', mean_squared_error(y, yhat)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared by hand: it is 1 minus the % variance explained\n",
    "# so, 1 - sum square residuals / variance in y\n",
    "# code from: https://stackoverflow.com/questions/42033720/python-sklearn-multiple-linear-regression-display-r-squared\n",
    "SS_Residual = sum((y-yhat)**2)       \n",
    "SS_Total = sum((y-np.mean(y))**2)     \n",
    "r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is the R-squared so low, and why are the fitted values so large?\n",
    "\n",
    "> We needed to winsorize the data! Outliers have a very large influence on coefficients because in OLS the sum of squared residuals is being minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mstats \n",
    "\n",
    "# add winsorized variables to dataframe\n",
    "df['mtb_w'] = mstats.winsorize(df['mtb'], limits=[0.01, 0.01]).data\n",
    "df['roa_w'] = mstats.winsorize(df['roa'], limits=[0.01, 0.01]).data\n",
    "df['size_w'] = mstats.winsorize(df['size'], limits=[0.01, 0.01]).data\n",
    "\n",
    "x = df[['roa_w', 'size_w']]\n",
    "y = df['mtb_w']\n",
    "lm.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('intercept', lm.intercept_, 'coefficients', lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance\n",
    "print('R-squared:', r2_score(y, lm.predict( x )) )\n",
    "print('MSE:', mean_squared_error(y, lm.predict( x ))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS with Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "lm = smf.ols(\"mtb_w ~ roa_w + size_w\", data=df).fit() \n",
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.params)\n",
    "print('R-squared', lm.rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(lm.summary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here for an overview of properties that are available after a regression is estimated: https://tedboy.github.io/statsmodels_doc/generated/generated/statsmodels.regression.linear_model.RegressionResults.html\n",
    "        \n",
    "For example: mse_resid - Mean squared error of the residuals. The sum of squared residuals divided by the residual degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.mse_resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vs Test Set\n",
    "\n",
    "Depending on the setting, sometimes the regression is estimated using the full sample. In that case all observations are used to train the model.\n",
    "\n",
    "It is also possible to split the sample into a training dataset and a testing dataset. The training sample is used to estimate the model, and the model is evaluated by comparing the predicted values on the test set with the actual values. \n",
    "\n",
    "For documentation of sklearn's train_test_split, see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df[['roa_w', 'size_w']]\n",
    "y = df['mtb_w']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.40, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(np.random.randn( len(df), 1))\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the sample by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sample by hand\n",
    "# msk is a list of booleans (True, False) with the same length as the df\n",
    "# 60% of the random numbers will be True (less than 0.6)\n",
    "msk = np.random.rand(len(df)) < 0.6\n",
    "print( len(msk), msk[0:20] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map all the True values into the training sample\n",
    "x_train = x[ msk ]\n",
    "y_train = y[ msk ]\n",
    "# the remainder into the test sample (~ means Not)\n",
    "x_test = x[ ~msk ]\n",
    "y_test = y[ ~msk ]\n",
    "print('#obs in training sample', len(x_train), '#obs in test sample:', len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model with the training data\n",
    "lm = LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "lm.predict( x )\n",
    "yhat_test = lm.predict(x_test)\n",
    "\n",
    "print('R-squared:', r2_score(y_test, yhat_test) )\n",
    "print('MSE:', mean_squared_error(y_test, yhat_test) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see: \n",
    "    \n",
    "- Sklearn - Linear Regression Example - https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n",
    "- Getting r-squared from sklearn: https://stackoverflow.com/questions/42033720/python-sklearn-multiple-linear-regression-display-r-squared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
